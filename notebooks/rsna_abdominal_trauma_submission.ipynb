{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fdb808",
   "metadata": {
    "papermill": {
     "duration": 0.006106,
     "end_time": "2023-10-13T15:16:43.425531",
     "exception": false,
     "start_time": "2023-10-13T15:16:43.419425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e56468",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-13T15:16:43.438861Z",
     "iopub.status.busy": "2023-10-13T15:16:43.438553Z",
     "iopub.status.idle": "2023-10-13T15:17:16.691535Z",
     "shell.execute_reply": "2023-10-13T15:17:16.690486Z"
    },
    "papermill": {
     "duration": 33.262275,
     "end_time": "2023-10-13T15:17:16.693742",
     "exception": false,
     "start_time": "2023-10-13T15:16:43.431467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rsna-atd-packages/dicomsdl-0.109.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\r\n",
      "Installing collected packages: dicomsdl\r\n",
      "Successfully installed dicomsdl-0.109.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rsna-atd-packages/dicomsdl-0.109.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc071f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:16.707925Z",
     "iopub.status.busy": "2023-10-13T15:17:16.707603Z",
     "iopub.status.idle": "2023-10-13T15:17:59.579167Z",
     "shell.execute_reply": "2023-10-13T15:17:59.578081Z"
    },
    "papermill": {
     "duration": 42.88143,
     "end_time": "2023-10-13T15:17:59.581373",
     "exception": false,
     "start_time": "2023-10-13T15:17:16.699943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/kaggle/input/pretrained-models-pytorch/pretrainedmodels/models/dpn.py:255: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if block_type is 'proj':\n",
      "/kaggle/input/pretrained-models-pytorch/pretrainedmodels/models/dpn.py:258: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif block_type is 'down':\n",
      "/kaggle/input/pretrained-models-pytorch/pretrainedmodels/models/dpn.py:262: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  assert block_type is 'normal'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/input/efficientnet-pytorch\")\n",
    "sys.path.append(\"/kaggle/input/monai-v101\")\n",
    "sys.path.append(\"/kaggle/input/pretrained-models-pytorch\")\n",
    "sys.path.append(\"/kaggle/input/rsna-atd-packages\")\n",
    "sys.path.append(\"/kaggle/input/smp-github/segmentation_models.pytorch-master\")\n",
    "\n",
    "import albumentations as A\n",
    "import attention\n",
    "import conv3d_same\n",
    "import cv2\n",
    "import dicomsdl as dsdl\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pydicom as dicom\n",
    "import pytorch_lightning as pl\n",
    "import random\n",
    "import timm\n",
    "import timm.models.layers as layers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import yaml\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from ast import literal_eval\n",
    "from functools import partial\n",
    "from heads import ClassificationHead\n",
    "from monai.transforms import Resize\n",
    "from segmentation_models_pytorch.base.initialization import initialize_decoder, initialize_head\n",
    "from sklearn.metrics import log_loss\n",
    "from timm import create_model\n",
    "from timm.models.layers import Conv2dSame\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f6e288",
   "metadata": {
    "papermill": {
     "duration": 0.005689,
     "end_time": "2023-10-13T15:17:59.593201",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.587512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654acbd",
   "metadata": {
    "papermill": {
     "duration": 0.005767,
     "end_time": "2023-10-13T15:17:59.604754",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.598987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2197c981",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.618491Z",
     "iopub.status.busy": "2023-10-13T15:17:59.617764Z",
     "iopub.status.idle": "2023-10-13T15:17:59.639863Z",
     "shell.execute_reply": "2023-10-13T15:17:59.639031Z"
    },
    "papermill": {
     "duration": 0.031243,
     "end_time": "2023-10-13T15:17:59.641718",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.610475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_activation(activation):\n",
    "    if activation is None:\n",
    "        return nn.Identity()\n",
    "    elif activation == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation == \"silu\":\n",
    "        return nn.SiLU(inplace=True)\n",
    "    else:\n",
    "        raise ValueError(f\"Activation {activation} is not supported.\")\n",
    "\n",
    "\n",
    "class SeparableConv2d(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride=1,\n",
    "        padding=0,\n",
    "        dilation=1,\n",
    "        bias=True,\n",
    "    ):\n",
    "        dephtwise_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            groups=in_channels,\n",
    "            bias=False,\n",
    "        )\n",
    "        pointwise_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=bias,\n",
    "        )\n",
    "        super().__init__(dephtwise_conv, pointwise_conv)\n",
    "\n",
    "\n",
    "class SeparableConvBnAct(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        dilation=1,\n",
    "        use_batchnorm=True, \n",
    "        activation=\"silu\"\n",
    "    ):\n",
    "        conv = SeparableConv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            dilation=dilation,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        bn = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
    "        act = get_activation(activation)\n",
    "        super(SeparableConvBnAct, self).__init__(conv, bn, act)\n",
    "\n",
    "\n",
    "class ConvBnAct(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        padding=0,\n",
    "        stride=1,\n",
    "        use_batchnorm=True,\n",
    "        activation=None\n",
    "    ):\n",
    "        conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            bias=not (use_batchnorm),\n",
    "        )\n",
    "        bn = nn.BatchNorm2d(out_channels) if use_batchnorm else nn.Identity()\n",
    "        act = get_activation(activation)\n",
    "        super(ConvBnAct, self).__init__(conv, bn, act)\n",
    "\n",
    "\n",
    "class ASPPPooling(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, activation=\"silu\"):\n",
    "        super().__init__(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            ConvBnAct(in_channels, out_channels, kernel_size=1, activation=activation)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.shape[-2:]\n",
    "        for module in self:\n",
    "            x = module(x)\n",
    "        return F.interpolate(x, size=size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels,\n",
    "        out_channels, \n",
    "        atrous_rates, \n",
    "        reduction=1,\n",
    "        dropout=0.2, \n",
    "        activation=\"silu\"\n",
    "    ):\n",
    "        super(ASPP, self).__init__()\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            ConvBnAct(\n",
    "                in_channels, \n",
    "                out_channels // reduction, \n",
    "                kernel_size=1, \n",
    "                padding=0,\n",
    "                stride=1,\n",
    "                use_batchnorm=True,\n",
    "                activation=activation\n",
    "            )\n",
    "        )\n",
    "        for r in atrous_rates:\n",
    "            modules.append(\n",
    "                SeparableConvBnAct(\n",
    "                in_channels, \n",
    "                out_channels // reduction, \n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=r,\n",
    "                dilation=r,\n",
    "                use_batchnorm=True,\n",
    "                activation=activation\n",
    "            ))\n",
    "        modules.append(ASPPPooling(in_channels, out_channels // reduction, activation=activation))\n",
    "        self.body = nn.ModuleList(modules)\n",
    "        self.project = nn.Sequential(\n",
    "            ConvBnAct(\n",
    "                (len(atrous_rates) + 2) * out_channels // reduction, \n",
    "                out_channels, \n",
    "                kernel_size=1, \n",
    "                padding=0, \n",
    "                stride=1,\n",
    "                use_batchnorm=True,\n",
    "                activation=activation\n",
    "            ),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, scale_factor=1):\n",
    "        if scale_factor != 1:\n",
    "            x = F.interpolate(x, scale_factor=scale_factor, mode=\"bilinear\")\n",
    "        results = []\n",
    "        for module in self.body:\n",
    "            results.append(module(x))\n",
    "        results = torch.cat(results, dim=1)\n",
    "        return self.project(results)\n",
    "\n",
    "\n",
    "class SegmentationHead(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels, \n",
    "        out_channels, \n",
    "        kernel_size=3, \n",
    "        padding=1, \n",
    "        upsampling=1\n",
    "    ):\n",
    "        blocks = [\n",
    "            ConvBnAct(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=1,\n",
    "                use_batchnorm=False,\n",
    "                activation=None\n",
    "            ),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
    "        ]\n",
    "        super(SegmentationHead, self).__init__(*blocks)\n",
    "\n",
    "\n",
    "class SCSEModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels // reduction, in_channels, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.sSE = nn.Sequential(nn.Conv2d(in_channels, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x * self.cSE(x) + x * self.sSE(x)) / 2.\n",
    "    \n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, name, **params):\n",
    "        super().__init__()\n",
    "        if name is None:\n",
    "            self.attention = nn.Identity(**params)\n",
    "        elif name == \"scse\":\n",
    "            self.attention = SCSEModule(**params)\n",
    "        else:\n",
    "            raise ValueError(\"Attention type {} is not implemented\".format(name))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247ec65b",
   "metadata": {
    "papermill": {
     "duration": 0.005807,
     "end_time": "2023-10-13T15:17:59.653447",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.647640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a51c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.666965Z",
     "iopub.status.busy": "2023-10-13T15:17:59.666471Z",
     "iopub.status.idle": "2023-10-13T15:17:59.675011Z",
     "shell.execute_reply": "2023-10-13T15:17:59.674080Z"
    },
    "papermill": {
     "duration": 0.017261,
     "end_time": "2023-10-13T15:17:59.676815",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.659554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_encoder(encoder_params):\n",
    "    module = getattr(sys.modules[__name__], encoder_params[\"class\"])\n",
    "    name = encoder_params[\"encoder_name\"]\n",
    "    return module(name=name, **encoder_params[\"params\"])\n",
    "\n",
    "\n",
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, out_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [nn.Identity()]\n",
    "\n",
    "    def forward(self, x):\n",
    "        stages = self.get_stages()\n",
    "        features = []\n",
    "        for stage in stages:\n",
    "            x = stage(x)\n",
    "            features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "class EfficientNetEncoder2d(BaseEncoder):\n",
    "    def __init__(\n",
    "        self, \n",
    "        name,\n",
    "        stage_idx,\n",
    "        backbone_params={},\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = create_model(name, **backbone_params)\n",
    "        assert len(stage_idx) <= len(self.encoder.blocks)\n",
    "        self.stage_idx = stage_idx\n",
    "        self.depth = len(stage_idx) + 2\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [nn.Identity(), nn.Sequential(self.encoder.conv_stem, self.encoder.bn1)] + \\\n",
    "            [self.encoder.blocks[i : j] for i, j in zip([0] + self.stage_idx, self.stage_idx + [len(self.encoder.blocks)])]\n",
    "    \n",
    "    def forward_head(self, x):\n",
    "        x = self.encoder.conv_head(x)\n",
    "        x = self.encoder.bn2(x)\n",
    "        x = self.encoder.forward_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0396b2",
   "metadata": {
    "papermill": {
     "duration": 0.006011,
     "end_time": "2023-10-13T15:17:59.688959",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.682948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c541c90f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.702259Z",
     "iopub.status.busy": "2023-10-13T15:17:59.701995Z",
     "iopub.status.idle": "2023-10-13T15:17:59.714695Z",
     "shell.execute_reply": "2023-10-13T15:17:59.713887Z"
    },
    "papermill": {
     "duration": 0.021528,
     "end_time": "2023-10-13T15:17:59.716505",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.694977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        skip_channels,\n",
    "        out_channels,\n",
    "        block_depth=1,\n",
    "        separable=False,\n",
    "        use_aspp=False,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        activation=\"relu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attention = nn.ModuleList([\n",
    "            Attention(attention_type, in_channels=in_channels + skip_channels),\n",
    "            Attention(attention_type, in_channels=out_channels)\n",
    "        ])\n",
    "        self.aspp = ASPP(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            atrous_rates=[1, 2, 4],\n",
    "            reduction=2,\n",
    "            dropout=0.2,\n",
    "            activation=activation\n",
    "        ) if use_aspp else nn.Identity()\n",
    "        module = SeparableConvBnAct if separable else ConvBnAct\n",
    "        self.stem = module(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_batchnorm=use_batchnorm,\n",
    "            activation=activation\n",
    "        )\n",
    "        self.body = nn.Sequential(*[\n",
    "            module(\n",
    "                out_channels, \n",
    "                out_channels, \n",
    "                kernel_size=3, \n",
    "                padding=1, \n",
    "                use_batchnorm=use_batchnorm,\n",
    "                activation=activation\n",
    "            ) for _ in range(block_depth)\n",
    "         ])\n",
    "\n",
    "    def forward(self, x, skip=None, scale_factor=1):\n",
    "        if scale_factor != 1:\n",
    "            x = F.interpolate(x, scale_factor=scale_factor, mode=\"trilinear\")\n",
    "        x = self.aspp(x)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention[0](x)\n",
    "        x = self.stem(x)\n",
    "        x = self.body(x)\n",
    "        x = self.attention[1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels,\n",
    "        decoder_channels,\n",
    "        scale_factors,\n",
    "        num_blocks=5,\n",
    "        block_depth=1,\n",
    "        separable=False,\n",
    "        use_aspp=False,\n",
    "        use_batchnorm=True,\n",
    "        attention_type=None,\n",
    "        activation=\"relu\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert num_blocks >= len(encoder_channels) - 1\n",
    "        assert num_blocks == len(decoder_channels)\n",
    "        assert num_blocks == len(scale_factors)\n",
    "        self.scale_factors = scale_factors\n",
    "        encoder_channels = encoder_channels[1:][::-1]\n",
    "        in_channels = [encoder_channels[0]] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:])\n",
    "        skip_channels += [0] * (len(in_channels) - len(skip_channels))\n",
    "        out_channels = decoder_channels\n",
    "        aspp_idx = len(in_channels) - 2\n",
    "        blocks = []\n",
    "        for i, (i_ch, s_ch, o_ch) in enumerate(zip(in_channels, skip_channels, out_channels)):\n",
    "            blocks.append(\n",
    "                DecoderBlock(\n",
    "                    i_ch, \n",
    "                    s_ch, \n",
    "                    o_ch, \n",
    "                    block_depth,\n",
    "                    separable=separable,\n",
    "                    use_aspp=use_aspp if i == aspp_idx else False,\n",
    "                    use_batchnorm=use_batchnorm, \n",
    "                    attention_type=attention_type,\n",
    "                    activation=activation\n",
    "                )\n",
    "            )\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, *features):\n",
    "        features = features[1:][::-1]\n",
    "        x = features[0]\n",
    "        skips = features[1:]\n",
    "        for i, (block, scale_factor) in enumerate(zip(self.blocks, self.scale_factors)):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = block(x, skip, scale_factor)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f75735",
   "metadata": {
    "papermill": {
     "duration": 0.006007,
     "end_time": "2023-10-13T15:17:59.728331",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.722324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5efbab96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.742067Z",
     "iopub.status.busy": "2023-10-13T15:17:59.741785Z",
     "iopub.status.idle": "2023-10-13T15:17:59.754052Z",
     "shell.execute_reply": "2023-10-13T15:17:59.753113Z"
    },
    "papermill": {
     "duration": 0.021339,
     "end_time": "2023-10-13T15:17:59.755796",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.734457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_segmentation_model(config):\n",
    "    config_ = config.copy()\n",
    "    family = config_.pop(\"family\")\n",
    "    if family == \"unet\":\n",
    "        return inflate_module(Unet(**config_))\n",
    "    else:\n",
    "        raise ValueError(f\"Model family {family} is not supported.\")\n",
    "    \n",
    "\n",
    "def inflate_module(module):\n",
    "    module_ = module\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        module_ = nn.BatchNorm3d(\n",
    "            module.num_features,\n",
    "            module.eps,\n",
    "            module.momentum,\n",
    "            module.affine,\n",
    "            module.track_running_stats,\n",
    "        )\n",
    "        if module.affine:\n",
    "            with torch.no_grad():\n",
    "                module_.weight = module.weight\n",
    "                module_.bias = module.bias\n",
    "        module_.running_mean = module.running_mean\n",
    "        module_.running_var = module.running_var\n",
    "        module_.num_batches_tracked = module.num_batches_tracked\n",
    "        if hasattr(module, \"qconfig\"):\n",
    "            module_.qconfig = module.qconfig\n",
    "    elif isinstance(module, Conv2dSame):\n",
    "        module_ = conv3d_same.Conv3dSame(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "        )\n",
    "        module_.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1, 1, 1, 1,module.kernel_size[0]))\n",
    "    elif isinstance(module, torch.nn.Conv2d):\n",
    "        module_ = torch.nn.Conv3d(\n",
    "            in_channels=module.in_channels,\n",
    "            out_channels=module.out_channels,\n",
    "            kernel_size=module.kernel_size[0],\n",
    "            stride=module.stride[0],\n",
    "            padding=module.padding[0],\n",
    "            dilation=module.dilation[0],\n",
    "            groups=module.groups,\n",
    "            bias=module.bias is not None,\n",
    "            padding_mode=module.padding_mode\n",
    "        )\n",
    "        module_.weight = torch.nn.Parameter(module.weight.unsqueeze(-1).repeat(1, 1, 1, 1,module.kernel_size[0]))\n",
    "    elif isinstance(module, torch.nn.MaxPool2d):\n",
    "        module_ = torch.nn.MaxPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            dilation=module.dilation,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    elif isinstance(module, torch.nn.AvgPool2d):\n",
    "        module_ = torch.nn.AvgPool3d(\n",
    "            kernel_size=module.kernel_size,\n",
    "            stride=module.stride,\n",
    "            padding=module.padding,\n",
    "            ceil_mode=module.ceil_mode,\n",
    "        )\n",
    "    for name, child in module.named_children():\n",
    "        module_.add_module(name, inflate_module(child))\n",
    "    del module\n",
    "    return module_\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        encoder_params,\n",
    "        decoder_params, \n",
    "        num_classes=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = create_encoder(encoder_params)\n",
    "        self.decoder = UnetDecoder(self.encoder.out_channels, **decoder_params)\n",
    "        self.head = SegmentationHead(\n",
    "            decoder_params[\"decoder_channels\"][-1], \n",
    "            num_classes, \n",
    "            kernel_size=3,\n",
    "            padding=1, \n",
    "            upsampling=1\n",
    "        )\n",
    "        initialize_decoder(self.decoder)\n",
    "        initialize_head(self.head)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(*features)\n",
    "        logits = self.head(decoder_output)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "class SegmentationModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = create_segmentation_model(config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276554bd",
   "metadata": {
    "papermill": {
     "duration": 0.005681,
     "end_time": "2023-10-13T15:17:59.767167",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.761486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Slice Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98f497e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.780046Z",
     "iopub.status.busy": "2023-10-13T15:17:59.779774Z",
     "iopub.status.idle": "2023-10-13T15:17:59.786940Z",
     "shell.execute_reply": "2023-10-13T15:17:59.786038Z"
    },
    "papermill": {
     "duration": 0.015657,
     "end_time": "2023-10-13T15:17:59.788599",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.772942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_slice_classification_model(encoder_params):\n",
    "    module = getattr(sys.modules[__name__], encoder_params[\"class\"])\n",
    "    name = encoder_params[\"encoder_name\"]\n",
    "    return module(name=name, **encoder_params[\"params\"])\n",
    "\n",
    "\n",
    "class SliceClassificationModel(nn.Module):\n",
    "    def __init__(self, name, backbone_params, dropout):\n",
    "        super().__init__()\n",
    "        self.num_channels = backbone_params[\"in_chans\"]\n",
    "        self.encoder = create_model(name, num_classes=0, **backbone_params)\n",
    "        feature_dim = self.feature_dim()\n",
    "        self.drop = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.head = ClassificationHead(feature_dim, (2, 3, 4, 4, 4, 5))\n",
    "\n",
    "    def feature_dim(self):\n",
    "        x = torch.randn(2, self.num_channels, 256, 256)\n",
    "        return self.encoder(x).shape[-1]\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.drop(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "    \n",
    "class SliceClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = create_slice_classification_model(config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ebd21e",
   "metadata": {
    "papermill": {
     "duration": 0.011683,
     "end_time": "2023-10-13T15:17:59.810578",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.798895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Scan Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6644fd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.823674Z",
     "iopub.status.busy": "2023-10-13T15:17:59.823391Z",
     "iopub.status.idle": "2023-10-13T15:17:59.831016Z",
     "shell.execute_reply": "2023-10-13T15:17:59.830070Z"
    },
    "papermill": {
     "duration": 0.016431,
     "end_time": "2023-10-13T15:17:59.832921",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.816490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_scan_classification_model(config):\n",
    "    return ScanClassificationModel(**config)\n",
    "\n",
    "\n",
    "class ScanClassificationModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        time_dim, \n",
    "        feature_dim, \n",
    "        hidden_dim, \n",
    "        num_layers=1,\n",
    "        dropout=0.2,\n",
    "        bidirectional=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.GRU(\n",
    "            feature_dim, \n",
    "            hidden_dim, \n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        scale_factor = 2 if bidirectional else 1\n",
    "        self.attention = attention.Attention(time_dim, hidden_dim * scale_factor)\n",
    "        self.drop = nn.Dropout(dropout) if dropout > 0.0 else nn.Identity()\n",
    "        self.seg_head = ClassificationHead(scale_factor * hidden_dim, [5])\n",
    "        self.clf_head = ClassificationHead(scale_factor * hidden_dim * 2, [2, 2, 3, 3, 3])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.drop(x)\n",
    "        logits_list = self.seg_head(x * torch.unsqueeze(mask, dim=-1))\n",
    "        max_pool, _ = torch.max(x, dim=1)\n",
    "        att_pool = self.attention(x, mask)\n",
    "        cat = torch.cat([max_pool, att_pool], dim=1)\n",
    "        logits_list = self.clf_head(cat) + logits_list\n",
    "        return logits_list\n",
    "    \n",
    "    \n",
    "class ScanClassificationModule(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model = create_scan_classification_model(config[\"model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f5f35",
   "metadata": {
    "papermill": {
     "duration": 0.005637,
     "end_time": "2023-10-13T15:17:59.844121",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.838484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e3c4ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.857382Z",
     "iopub.status.busy": "2023-10-13T15:17:59.856521Z",
     "iopub.status.idle": "2023-10-13T15:17:59.861267Z",
     "shell.execute_reply": "2023-10-13T15:17:59.860468Z"
    },
    "papermill": {
     "duration": 0.013375,
     "end_time": "2023-10-13T15:17:59.863066",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.849691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"/kaggle/input/rsna-2023-abdominal-trauma-detection\"\n",
    "SEGMENTATION_DIM = [128, 224, 224]\n",
    "INJURY_CATEGORIES = [\n",
    "    \"any\",\n",
    "    \"extravasation\",\n",
    "    \"bowel\",\n",
    "    \"liver\",\n",
    "    \"spleen\",\n",
    "    \"kidney\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379a5ee4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.876172Z",
     "iopub.status.busy": "2023-10-13T15:17:59.875929Z",
     "iopub.status.idle": "2023-10-13T15:17:59.885186Z",
     "shell.execute_reply": "2023-10-13T15:17:59.884227Z"
    },
    "papermill": {
     "duration": 0.017993,
     "end_time": "2023-10-13T15:17:59.886886",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.868893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rescale_slice_to_array_dicomsdl(slice):\n",
    "    image = slice.pixelData(storedvalue=True)\n",
    "    info = slice.getPixelDataInfo()\n",
    "    if slice.PixelRepresentation == 1:\n",
    "        bit_shift = slice.BitsAllocated - slice.BitsStored\n",
    "        dtype = image.dtype\n",
    "        image = (image << bit_shift).astype(dtype) >> bit_shift\n",
    "    slope, intercept = 1.0, 0.0\n",
    "    center, width = 50, 400\n",
    "    if \"RescaleSlope\" in info and \"RescaleIntercept\" in info:\n",
    "        slope, intercept = slice.RescaleSlope, slice.RescaleIntercept\n",
    "    if \"WindowCenter\" in info and \"WindowWidth\" in info:\n",
    "        center, width = slice.WindowCenter, slice.WindowWidth\n",
    "    low, high = center - width / 2, center + width / 2\n",
    "    image = np.clip(image * slope + intercept, low, high)\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_min_max(image, eps=1.0e-8):\n",
    "    return (image - image.min()) / (image.max() - image.min() + eps)\n",
    "\n",
    "\n",
    "def convert_to_uint8(image):\n",
    "    return (image * 255.).astype(np.uint8)\n",
    "\n",
    "\n",
    "def resize_volume(x):\n",
    "    x = Resize(SEGMENTATION_DIM, mode=\"trilinear\")(x).numpy().astype(np.uint8)\n",
    "    return x\n",
    "\n",
    "\n",
    "def flip_scan(slices):\n",
    "    x0, x1 = slices\n",
    "    if x1.ImagePositionPatient[2] > x0.ImagePositionPatient[2]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_scan_path(patient_id, series_id, split=\"train\"):\n",
    "    return os.path.join(DATA_DIR, f\"{split}_images\", str(patient_id), str(series_id))\n",
    "\n",
    "\n",
    "def get_slice_path(scan_path, slice_idx):\n",
    "    return os.path.join(scan_path, f\"{slice_idx}.dcm\")\n",
    "\n",
    "\n",
    "def get_sorted_slices(scan_path):\n",
    "    return sorted([int(p.split(\".\")[0]) for p in os.listdir(scan_path)])\n",
    "\n",
    "\n",
    "def sample_indices(n, m):\n",
    "    return np.quantile(list(range(n)), np.linspace(0., 1., m)).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e152d7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.900132Z",
     "iopub.status.busy": "2023-10-13T15:17:59.899629Z",
     "iopub.status.idle": "2023-10-13T15:17:59.917794Z",
     "shell.execute_reply": "2023-10-13T15:17:59.915737Z"
    },
    "papermill": {
     "duration": 0.028438,
     "end_time": "2023-10-13T15:17:59.921139",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.892701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_step_size(n):\n",
    "    step = 1\n",
    "    if n > 256:\n",
    "        step = 2\n",
    "    if n > 512:\n",
    "        step = 4\n",
    "    return step\n",
    "\n",
    "\n",
    "def load_scan(scan_path):\n",
    "    sorted_slices = get_sorted_slices(scan_path)\n",
    "    step = get_step_size(len(sorted_slices))\n",
    "    slices = []\n",
    "    for i in sorted_slices[::step]:\n",
    "        try:\n",
    "            slices.append(dsdl.open(get_slice_path(scan_path, i)))\n",
    "        except:\n",
    "            pass\n",
    "    flip = False\n",
    "    if len(slices) > 1:\n",
    "        flip = flip_scan(slices[:2])\n",
    "    if flip:\n",
    "        slices = slices[::-1]\n",
    "    image = []\n",
    "    for slice in slices:\n",
    "        try:\n",
    "            image.append(rescale_slice_to_array_dicomsdl(slice))\n",
    "        except:\n",
    "            pass\n",
    "    if len(image) == 0:\n",
    "        image = np.zeros(128, 256, 256)\n",
    "    else:\n",
    "        image = np.stack(image, axis=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def clean_bounds(bounds):\n",
    "    x0, x1 = min([b[0][0] for b in bounds]), max([b[0][1] for b in bounds])\n",
    "    y0, y1 = min([b[1][0] for b in bounds]), max([b[1][1] for b in bounds])\n",
    "    return (x0, x1), (y0, y1)\n",
    "\n",
    "\n",
    "def crop(x, bounds, i):\n",
    "    (x0, x1), (y0, y1) = clean_bounds([b[i] for b in bounds])\n",
    "    return x[x0 : x1, y0 : y1]\n",
    "\n",
    "\n",
    "def resize_slice(image, image_size, numpy=True):\n",
    "    if numpy:\n",
    "        image = A.Compose([\n",
    "            A.Resize(image_size, image_size, interpolation=cv2.INTER_LINEAR, always_apply=True)\n",
    "        ])(image=image)[\"image\"]\n",
    "    else:\n",
    "        image = F.interpolate(image[None, None, :], size=(image_size, image_size), mode=\"bilinear\")[0, 0]\n",
    "    return image\n",
    "\n",
    "\n",
    "def normalize_slice(image, num_channels=3):\n",
    "    tf = A.Compose([\n",
    "        A.Normalize(mean=[0.5] * num_channels, std=[0.5] * num_channels, max_pixel_value=1.0, always_apply=True),\n",
    "        ToTensorV2(always_apply=True)\n",
    "    ])(image=image)\n",
    "    image = tf[\"image\"]\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_segmentation(image, device=\"cuda\"):\n",
    "    if len(image) < SEGMENTATION_DIM[0]:\n",
    "        image = np.stack([image[i] for i in sample_indices(len(image), SEGMENTATION_DIM[0])])\n",
    "    image = convert_to_uint8(normalize_min_max(image))\n",
    "    if image.ndim < 4:\n",
    "        image = image[None, :].repeat(3, 0)\n",
    "    image = resize_volume(image)\n",
    "    image = image / 127.5 - 1.0\n",
    "    image = torch.tensor(image).float()[None, :].to(device)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_slice(image, mask, bounds, image_size=384, num_channels=3, device=\"cuda\"):\n",
    "    slices = []\n",
    "    indices = range(len(image)) if len(image) < 256 else sample_indices(len(image), 256)\n",
    "    for i in indices:\n",
    "        x = image[i]\n",
    "        x = normalize_min_max(x)\n",
    "        x = crop(x, bounds, i=1)\n",
    "        x = resize_slice(x, image_size)\n",
    "        x = x[None, :].repeat(num_channels, 0)\n",
    "        x = np.transpose(x, (1, 2, 0))\n",
    "        x = normalize_slice(x, num_channels=num_channels).to(device)\n",
    "        z = min(int(i * SEGMENTATION_DIM[0] / len(image)), SEGMENTATION_DIM[0] - 1)\n",
    "        m = mask[1:, z].max(dim=0)[0]\n",
    "        m = crop(m, bounds, i=0)\n",
    "        m = resize_slice(m, image_size, numpy=False)\n",
    "        m = 2.0 * m - 1.0\n",
    "        x = torch.cat([x, m[None, :]], axis=0)\n",
    "        slices.append(x)\n",
    "    image = torch.stack(slices, axis=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def preprocess_scan(features, time_dim, device=\"cuda\"):\n",
    "    t = features.shape[0]\n",
    "    attention_mask = torch.ones((time_dim,), dtype=torch.float32).to(device)\n",
    "    if t > time_dim:\n",
    "        x = F.interpolate(features[None, None, :], size=(time_dim, features.shape[-1]), mode=\"bilinear\")[0]\n",
    "    else:\n",
    "        pad_dim = (0, 0, 0, time_dim - t)\n",
    "        x = F.pad(features, pad_dim, mode=\"constant\", value=0.0)[None, :]\n",
    "        attention_mask[t:] = 0.0\n",
    "    attention_mask = attention_mask[None, :]\n",
    "    return x, attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29931a3e",
   "metadata": {
    "papermill": {
     "duration": 0.008476,
     "end_time": "2023-10-13T15:17:59.943450",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.934974",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8571b247",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:17:59.980728Z",
     "iopub.status.busy": "2023-10-13T15:17:59.980212Z",
     "iopub.status.idle": "2023-10-13T15:18:00.007034Z",
     "shell.execute_reply": "2023-10-13T15:18:00.005702Z"
    },
    "papermill": {
     "duration": 0.047421,
     "end_time": "2023-10-13T15:18:00.009811",
     "exception": false,
     "start_time": "2023-10-13T15:17:59.962390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_segmentation_bounds(n, s):\n",
    "    nx, ny, nz = n\n",
    "    sx, sy, sz = s\n",
    "    m = len(nx)\n",
    "    if m == 0:\n",
    "        x0, x1 = 0, 0\n",
    "        y0, y1 = 0, 0\n",
    "        z0, z1 = 0, 0\n",
    "    else:\n",
    "        x0, x1 = int(0.8 * nx[int(0.001 * m)]), int(1.2 * nx[int(0.999 * m)])\n",
    "        y0, y1 = int(0.8 * ny[int(0.001 * m)]), int(1.2 * ny[int(0.999 * m)])\n",
    "        z0, z1 = int(nz[int(0.001 * m)]), int(nz[int(0.999 * m)])\n",
    "\n",
    "    xx0, xx1 = int(x0 * sx / SEGMENTATION_DIM[1]), int(x1 * sx / SEGMENTATION_DIM[1])\n",
    "    yy0, yy1 = int(y0 * sy / SEGMENTATION_DIM[2]), int(y1 * sy / SEGMENTATION_DIM[2])\n",
    "    zz0, zz1 = int(z0 * sz / SEGMENTATION_DIM[0]), int(z1 * sz / SEGMENTATION_DIM[0])\n",
    "\n",
    "    x0, x1 = max(x0, 0), min(x1, SEGMENTATION_DIM[1])\n",
    "    y0, y1 = max(y0, 0), min(y1, SEGMENTATION_DIM[2])\n",
    "    z0, z1 = max(z0, 0), min(z1, SEGMENTATION_DIM[0])\n",
    "\n",
    "    xx0, xx1 = max(xx0, 0), min(xx1, sx)\n",
    "    yy0, yy1 = max(yy0, 0), min(yy1, sy)\n",
    "    zz0, zz1 = max(zz0, 0), min(zz1, sz)\n",
    "    return [(x0, x1), (y0, y1), (z0, z1)], [(xx0, xx1), (yy0, yy1), (zz0, zz1)]\n",
    "\n",
    "\n",
    "def predict_segmentation(models, image, sample=1, device=\"cuda\"):\n",
    "    models = random.sample(models, sample)\n",
    "    sz = len(image)\n",
    "    sx, sy = image.shape[1], image.shape[2]\n",
    "    x = preprocess_segmentation(image, device=device)\n",
    "    with torch.no_grad():\n",
    "        logits = sum([m(x) for m in models]) / len(models)\n",
    "        probs = torch.sigmoid(logits)[0]\n",
    "    bounds = []\n",
    "    for organ_id in range(1, 6):\n",
    "        p0 = probs[organ_id] > 0.3\n",
    "        p1 = probs[organ_id] > 0.1\n",
    "        nz, nx, ny = torch.nonzero(p0, as_tuple=True)\n",
    "        n = len(nx)\n",
    "        if n == 0:\n",
    "            nz, nx, ny = torch.nonzero(p1, as_tuple=True)\n",
    "        nx, ny, nz = torch.sort(nx)[0], torch.sort(ny)[0], torch.sort(nz)[0]\n",
    "        b0, b1 = get_segmentation_bounds((nx, ny, nz), (sx, sy, sz))\n",
    "        bounds.append([b0, b1])\n",
    "    return probs, bounds\n",
    "\n",
    "\n",
    "def predict_slice(models, image, mask, bounds, image_size=384, num_channels=3, batch_size=16, sample=3, device=\"cuda\"):\n",
    "    models = random.sample(models, sample)\n",
    "    x = preprocess_slice(image, mask, bounds, image_size=image_size, num_channels=num_channels, device=device)\n",
    "    features = []\n",
    "    for x_ in torch.split(x, batch_size, dim=0):\n",
    "        with torch.no_grad():\n",
    "            f = sum([m.forward_features(x_) for m in models]) / len(models)\n",
    "        features.append(f)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    return features\n",
    "\n",
    "\n",
    "def predict_scan(models, features, time_dim=256, sample=5, device=\"cuda\"):\n",
    "    models = random.sample(models, sample)\n",
    "    x, attention_mask = preprocess_scan(features, time_dim=time_dim, device=device)\n",
    "    predictions = [[] for _ in range(5)]\n",
    "    for model in models:\n",
    "        with torch.no_grad():\n",
    "            logits_list = model(x, attention_mask)[:-1]\n",
    "        probs_list = [F.softmax(logits, dim=-1) for logits in logits_list]\n",
    "        for i, probs in enumerate(probs_list):\n",
    "            predictions[i].append(probs[0].cpu().numpy())\n",
    "    predictions = [sum(p) / len(models) for p in predictions]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54777cc",
   "metadata": {
    "papermill": {
     "duration": 0.005859,
     "end_time": "2023-10-13T15:18:00.023843",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.017984",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba3a26a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.036896Z",
     "iopub.status.busy": "2023-10-13T15:18:00.036544Z",
     "iopub.status.idle": "2023-10-13T15:18:00.041758Z",
     "shell.execute_reply": "2023-10-13T15:18:00.040834Z"
    },
    "papermill": {
     "duration": 0.014155,
     "end_time": "2023-10-13T15:18:00.043747",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.029592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/kaggle/working/configs/segmentation\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/configs/stage_1\", exist_ok=True)\n",
    "os.makedirs(\"/kaggle/working/configs/stage_2\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ebfa54b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.057745Z",
     "iopub.status.busy": "2023-10-13T15:18:00.057356Z",
     "iopub.status.idle": "2023-10-13T15:18:00.063480Z",
     "shell.execute_reply": "2023-10-13T15:18:00.062557Z"
    },
    "papermill": {
     "duration": 0.014874,
     "end_time": "2023-10-13T15:18:00.065259",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.050385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/segmentation/tf_efficientnetv2_s_128_224_224.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/segmentation/tf_efficientnetv2_s_128_224_224.yaml\n",
    "type: \"segmentation\"\n",
    "model:\n",
    "    family: \"unet\"\n",
    "    num_classes: 6\n",
    "    encoder_params:\n",
    "        class: EfficientNetEncoder2d\n",
    "        encoder_name: \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "        params:\n",
    "            out_channels: [3, 24, 48, 64, 160, 256]\n",
    "            stage_idx: [2, 3, 5]\n",
    "            backbone_params: \n",
    "                pretrained: false\n",
    "                in_chans: 3\n",
    "                drop_path_rate: 0.2\n",
    "    decoder_params: \n",
    "        decoder_channels: [256, 128, 64, 32, 16]\n",
    "        scale_factors: [2, 2, 2, 2, 2]\n",
    "        num_blocks: 5\n",
    "        block_depth: 1\n",
    "        separable: false\n",
    "        use_aspp: false\n",
    "        use_batchnorm: true \n",
    "        attention_type: \"scse\"\n",
    "        activation: \"silu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86848cc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.079822Z",
     "iopub.status.busy": "2023-10-13T15:18:00.078458Z",
     "iopub.status.idle": "2023-10-13T15:18:00.085017Z",
     "shell.execute_reply": "2023-10-13T15:18:00.083899Z"
    },
    "papermill": {
     "duration": 0.015875,
     "end_time": "2023-10-13T15:18:00.086813",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.070938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/stage_1/tf_efficientnetv2_s_384.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/stage_1/tf_efficientnetv2_s_384.yaml\n",
    "type: \"stage_1\"\n",
    "model:\n",
    "    class: SliceClassificationModel\n",
    "    encoder_name: \"tf_efficientnetv2_s.in21k_ft_in1k\"\n",
    "    params:\n",
    "        dropout: 0.2\n",
    "        backbone_params: \n",
    "            pretrained: false\n",
    "            in_chans: 4\n",
    "            drop_path_rate: 0.2\n",
    "data:\n",
    "    image_size: 384\n",
    "    num_channels: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4c5177",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.099710Z",
     "iopub.status.busy": "2023-10-13T15:18:00.099290Z",
     "iopub.status.idle": "2023-10-13T15:18:00.104754Z",
     "shell.execute_reply": "2023-10-13T15:18:00.103718Z"
    },
    "papermill": {
     "duration": 0.013805,
     "end_time": "2023-10-13T15:18:00.106453",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.092648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/stage_1/convnextv2_tiny_384.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/stage_1/convnextv2_tiny_384.yaml\n",
    "model:\n",
    "    class: SliceClassificationModel\n",
    "    encoder_name: \"convnextv2_tiny.fcmae_ft_in22k_in1k_384\"\n",
    "    params:\n",
    "        dropout: 0.2\n",
    "        backbone_params: \n",
    "            pretrained: false\n",
    "            in_chans: 4\n",
    "            drop_path_rate: 0.2\n",
    "data:\n",
    "    image_size: 384\n",
    "    num_channels: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b2b4a7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.119518Z",
     "iopub.status.busy": "2023-10-13T15:18:00.119037Z",
     "iopub.status.idle": "2023-10-13T15:18:00.124155Z",
     "shell.execute_reply": "2023-10-13T15:18:00.123375Z"
    },
    "papermill": {
     "duration": 0.013444,
     "end_time": "2023-10-13T15:18:00.125924",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.112480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/stage_1/maxxvitv2_nano_256.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/stage_1/maxxvitv2_nano_256.yaml\n",
    "model:\n",
    "    class: SliceClassificationModel\n",
    "    encoder_name: \"maxxvitv2_nano_rw_256.sw_in1k\"\n",
    "    params:\n",
    "        dropout: 0.2\n",
    "        backbone_params: \n",
    "            pretrained: false\n",
    "            in_chans: 2\n",
    "            drop_path_rate: 0.2\n",
    "data:\n",
    "    image_size: 256\n",
    "    num_channels: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04da1c08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.139600Z",
     "iopub.status.busy": "2023-10-13T15:18:00.138914Z",
     "iopub.status.idle": "2023-10-13T15:18:00.144625Z",
     "shell.execute_reply": "2023-10-13T15:18:00.143570Z"
    },
    "papermill": {
     "duration": 0.014287,
     "end_time": "2023-10-13T15:18:00.146273",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.131986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/stage_2/lstm_256_128.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/stage_2/lstm_256_128.yaml\n",
    "model:\n",
    "    time_dim: 256\n",
    "    feature_dim: 2816\n",
    "    hidden_dim: 128\n",
    "    num_layers: 2\n",
    "    dropout: 0.2\n",
    "    bidirectional: true\n",
    "data:\n",
    "    time_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29949668",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.160133Z",
     "iopub.status.busy": "2023-10-13T15:18:00.159515Z",
     "iopub.status.idle": "2023-10-13T15:18:00.166539Z",
     "shell.execute_reply": "2023-10-13T15:18:00.165487Z"
    },
    "papermill": {
     "duration": 0.015913,
     "end_time": "2023-10-13T15:18:00.168202",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.152289",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing configs/ensemble.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/ensemble.yaml\n",
    "device: \"cuda\"\n",
    "batch_size: 16\n",
    "\n",
    "models: \n",
    "    segmentation:\n",
    "        config_path: \"/kaggle/working/configs/segmentation/tf_efficientnetv2_s_128_224_224.yaml\"\n",
    "        checkpoint_paths: [\n",
    "            \"/kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_0.ckpt\",\n",
    "            \"/kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_1.ckpt\",\n",
    "            \"/kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_2.ckpt\",\n",
    "            \"/kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_3.ckpt\",\n",
    "            \"/kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_4.ckpt\"\n",
    "        ]\n",
    "    stage_1:\n",
    "        tf_efficientnetv2_s_384:\n",
    "            config_path: \"/kaggle/working/configs/stage_1/tf_efficientnetv2_s_384.yaml\"\n",
    "            checkpoint_paths: [\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_2.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_3.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_4.ckpt\"\n",
    "            ]\n",
    "        convnextv2_tiny_384:\n",
    "            config_path: \"/kaggle/working/configs/stage_1/convnextv2_tiny_384.yaml\"\n",
    "            checkpoint_paths: [\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_2.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_3.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_4.ckpt\"\n",
    "            ]\n",
    "        maxxvitv2_nano_256:\n",
    "            config_path: \"/kaggle/working/configs/stage_1/maxxvitv2_nano_256.yaml\"\n",
    "            checkpoint_paths: [\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_2.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_3.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_4.ckpt\"\n",
    "            ]\n",
    "    stage_2:\n",
    "        lstm_256_128:\n",
    "            config_path: \"/kaggle/working/configs/stage_2/lstm_256_128.yaml\"\n",
    "            checkpoint_paths: [\n",
    "                \"/kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_0.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_1.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_2.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_3.ckpt\",\n",
    "                \"/kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_4.ckpt\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1506509f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.230518Z",
     "iopub.status.busy": "2023-10-13T15:18:00.229976Z",
     "iopub.status.idle": "2023-10-13T15:18:00.238727Z",
     "shell.execute_reply": "2023-10-13T15:18:00.237802Z"
    },
    "papermill": {
     "duration": 0.017906,
     "end_time": "2023-10-13T15:18:00.240441",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.222535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(module, config, checkpoint_path):\n",
    "    print(f\"Checkpoint: {checkpoint_path}\")\n",
    "    model = module(config)\n",
    "    model.load_state_dict(torch.load(checkpoint_path)[\"state_dict\"])\n",
    "    m = model.model\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "\n",
    "def load_segmentation_models(config, device=\"cuda\"):\n",
    "    print(f\"Loading segmentation model...\")\n",
    "    with open(config[\"config_path\"], \"rb\") as f:\n",
    "        config_ = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    models = [load_model(SegmentationModule, config_, path).to(device) for path in config[\"checkpoint_paths\"]]\n",
    "    return models\n",
    "\n",
    "\n",
    "def load_stage_1_models(config, device=\"cuda\"):\n",
    "    models = {}\n",
    "    for name, params in config.items():\n",
    "        print(f\"Loading stage 1 family `{name}`...\")\n",
    "        with open(params[\"config_path\"], \"rb\") as f:\n",
    "            config_ = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        checkpoint_paths = params[\"checkpoint_paths\"]\n",
    "        models[name] = {\n",
    "            \"models\": [load_model(SliceClassificationModule, config_, path).to(device) for path in checkpoint_paths],\n",
    "            \"image_size\": config_[\"data\"][\"image_size\"],\n",
    "            \"num_channels\": config_[\"data\"][\"num_channels\"]\n",
    "        }\n",
    "    return models\n",
    "\n",
    "\n",
    "def load_stage_2_models(config, device=\"cuda\"):\n",
    "    models = {}\n",
    "    for name, params in config.items():\n",
    "        print(f\"Loading stage 2 family `{name}`...\")\n",
    "        with open(params[\"config_path\"], \"rb\") as f:\n",
    "            config_ = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        checkpoint_paths = params[\"checkpoint_paths\"]\n",
    "        models[name] = {\n",
    "            \"models\": [load_model(ScanClassificationModule, config_, path).to(device) for path in checkpoint_paths],\n",
    "            \"time_dim\": config_[\"data\"][\"time_dim\"]\n",
    "        }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca564d",
   "metadata": {
    "papermill": {
     "duration": 0.006099,
     "end_time": "2023-10-13T15:18:00.252791",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.246692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34568c78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:00.266756Z",
     "iopub.status.busy": "2023-10-13T15:18:00.266473Z",
     "iopub.status.idle": "2023-10-13T15:18:38.444400Z",
     "shell.execute_reply": "2023-10-13T15:18:38.443389Z"
    },
    "papermill": {
     "duration": 38.187305,
     "end_time": "2023-10-13T15:18:38.446151",
     "exception": false,
     "start_time": "2023-10-13T15:18:00.258846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading segmentation model...\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_0.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_1.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_2.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_3.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-segmentation/tf_efficientnetv2_s.in21k_ft_in1k__128_224_224__seed_0__fold_4.ckpt\n",
      "Loading stage 1 family `tf_efficientnetv2_s_384`...\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_2.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_3.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/tf_efficientnetv2_s.in21k_ft_in1k__image_size_384__seed_0__fold_4.ckpt\n",
      "Loading stage 1 family `convnextv2_tiny_384`...\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_2.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_3.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/convnextv2_tiny.fcmae_ft_in22k_in1k_384__image_size_384__seed_0__fold_4.ckpt\n",
      "Loading stage 1 family `maxxvitv2_nano_256`...\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_2.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_3.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-1/maxxvitv2_nano_rw_256.sw_in1k__image_size_256__seed_0__fold_4.ckpt\n",
      "Loading stage 2 family `lstm_256_128`...\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_0.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_1.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_2.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_3.ckpt\n",
      "Checkpoint: /kaggle/input/rsna-atd-final-stage-2/lstm_256_128__seed_0__fold_4.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23a882761a64da5810452528a542b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>kidney_healthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48843</td>\n",
       "      <td>0.479187</td>\n",
       "      <td>0.299890</td>\n",
       "      <td>0.309901</td>\n",
       "      <td>0.224372</td>\n",
       "      <td>0.328844</td>\n",
       "      <td>0.290889</td>\n",
       "      <td>0.195177</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.520813</td>\n",
       "      <td>0.700110</td>\n",
       "      <td>0.465727</td>\n",
       "      <td>0.380267</td>\n",
       "      <td>0.511123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50046</td>\n",
       "      <td>0.485541</td>\n",
       "      <td>0.374966</td>\n",
       "      <td>0.274808</td>\n",
       "      <td>0.247720</td>\n",
       "      <td>0.303287</td>\n",
       "      <td>0.243375</td>\n",
       "      <td>0.209228</td>\n",
       "      <td>0.208284</td>\n",
       "      <td>0.514459</td>\n",
       "      <td>0.625034</td>\n",
       "      <td>0.477472</td>\n",
       "      <td>0.453338</td>\n",
       "      <td>0.582488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63706</td>\n",
       "      <td>0.379059</td>\n",
       "      <td>0.444275</td>\n",
       "      <td>0.298089</td>\n",
       "      <td>0.196277</td>\n",
       "      <td>0.412387</td>\n",
       "      <td>0.228513</td>\n",
       "      <td>0.220229</td>\n",
       "      <td>0.190025</td>\n",
       "      <td>0.620941</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.505634</td>\n",
       "      <td>0.359100</td>\n",
       "      <td>0.589746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  extravasation_injury  bowel_injury  liver_low  liver_high  \\\n",
       "0       48843              0.479187      0.299890   0.309901    0.224372   \n",
       "1       50046              0.485541      0.374966   0.274808    0.247720   \n",
       "2       63706              0.379059      0.444275   0.298089    0.196277   \n",
       "\n",
       "   spleen_low  spleen_high  kidney_low  kidney_high  extravasation_healthy  \\\n",
       "0    0.328844     0.290889    0.195177     0.293700               0.520813   \n",
       "1    0.303287     0.243375    0.209228     0.208284               0.514459   \n",
       "2    0.412387     0.228513    0.220229     0.190025               0.620941   \n",
       "\n",
       "   bowel_healthy  liver_healthy  spleen_healthy  kidney_healthy  \n",
       "0       0.700110       0.465727        0.380267        0.511123  \n",
       "1       0.625034       0.477472        0.453338        0.582488  \n",
       "2       0.555725       0.505634        0.359100        0.589746  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = \"test\"\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, f\"{split}_series_meta.csv\"))\n",
    "\n",
    "with open(\"/kaggle/working/configs/ensemble.yaml\", \"rb\") as f:\n",
    "    CONFIG = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    \n",
    "segmentation = CONFIG[\"models\"][\"segmentation\"]\n",
    "stage_1 = CONFIG[\"models\"][\"stage_1\"]\n",
    "stage_2 = CONFIG[\"models\"][\"stage_2\"]\n",
    "device = CONFIG[\"device\"]\n",
    "batch_size = CONFIG[\"batch_size\"]\n",
    "\n",
    "segmentation_models = load_segmentation_models(segmentation, device=device)\n",
    "stage_1_models = load_stage_1_models(stage_1, device=device)\n",
    "stage_2_models = load_stage_2_models(stage_2, device=device)\n",
    "\n",
    "keys = []\n",
    "scan_paths = []\n",
    "for patient_id, series_id in list(df.groupby([\"patient_id\", \"series_id\"]).groups.keys()):\n",
    "    scan_path = get_scan_path(patient_id, series_id, split=split)\n",
    "    if not os.path.exists(scan_path):\n",
    "        continue\n",
    "    keys.append((patient_id, series_id))\n",
    "    scan_paths.append(scan_path)\n",
    "\n",
    "scan_predictions = [[] for _ in range(5)]\n",
    "for (patient_id, series_id), scan_path in tqdm(zip(keys, scan_paths)):\n",
    "    image = load_scan(scan_path)\n",
    "    mask, bounds = predict_segmentation(segmentation_models, image, device=device)\n",
    "    \n",
    "    features = []\n",
    "    for name in stage_1_models:\n",
    "        models = stage_1_models[name][\"models\"]\n",
    "        image_size = stage_1_models[name][\"image_size\"]\n",
    "        num_channels = stage_1_models[name][\"num_channels\"]\n",
    "        features.append(predict_slice(models, image, mask, bounds, image_size, num_channels, batch_size, device=device))\n",
    "    features = torch.cat(features, dim=1)\n",
    "    \n",
    "    predictions = []\n",
    "    for name in stage_2_models:\n",
    "        models = stage_2_models[name][\"models\"]\n",
    "        time_dim = stage_2_models[name][\"time_dim\"]\n",
    "        predictions.append(predict_scan(models, features, time_dim, device=device))\n",
    "    predictions = [sum([p[i] for p in predictions]) for i in range(5)]\n",
    "    \n",
    "    for i, p in enumerate(predictions):\n",
    "        scan_predictions[i].append(p)\n",
    "\n",
    "scan_predictions = np.concatenate([np.stack(p, axis=0) for p in scan_predictions], axis=1)\n",
    "df_submit = pd.DataFrame(keys, columns=[\"patient_id\", \"series_id\"])\n",
    "pos_cols = [\n",
    "    \"extravasation_injury\",\n",
    "    \"bowel_injury\",\n",
    "    \"liver_low\",\n",
    "    \"liver_high\",\n",
    "    \"spleen_low\",\n",
    "    \"spleen_high\",\n",
    "    \"kidney_low\",\n",
    "    \"kidney_high\"\n",
    "]\n",
    "neg_cols = [\n",
    "    \"extravasation_healthy\",\n",
    "    \"bowel_healthy\",\n",
    "    \"liver_healthy\",\n",
    "    \"spleen_healthy\",\n",
    "    \"kidney_healthy\"\n",
    "]\n",
    "target_cols = [\n",
    "    \"extravasation_healthy\",\n",
    "    \"extravasation_injury\",\n",
    "    \"bowel_healthy\",\n",
    "    \"bowel_injury\",\n",
    "    \"liver_healthy\",\n",
    "    \"liver_low\",\n",
    "    \"liver_high\",\n",
    "    \"spleen_healthy\",\n",
    "    \"spleen_low\",\n",
    "    \"spleen_high\",\n",
    "    \"kidney_healthy\",\n",
    "    \"kidney_low\",\n",
    "    \"kidney_high\",\n",
    "]\n",
    "df_submit[target_cols] = scan_predictions\n",
    "\n",
    "df_submit = df_submit.drop(columns=[\"series_id\"])\n",
    "df_submit_pos = df_submit[[\"patient_id\"] + pos_cols].groupby(\"patient_id\", as_index=False).max(numeric_only=True) # Max aggregation over injury columns\n",
    "df_submit_neg = df_submit[[\"patient_id\"] + neg_cols].groupby(\"patient_id\", as_index=False).min(numeric_only=True) # Min aggregation over healthy columns\n",
    "df_submit = df_submit_pos.merge(df_submit_neg, on=[\"patient_id\"], how=\"left\").reset_index(drop=True)\n",
    "df_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a544752a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:38.462335Z",
     "iopub.status.busy": "2023-10-13T15:18:38.462036Z",
     "iopub.status.idle": "2023-10-13T15:18:38.482334Z",
     "shell.execute_reply": "2023-10-13T15:18:38.481281Z"
    },
    "papermill": {
     "duration": 0.030261,
     "end_time": "2023-10-13T15:18:38.484364",
     "exception": false,
     "start_time": "2023-10-13T15:18:38.454103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>extravasation_injury</th>\n",
       "      <th>bowel_injury</th>\n",
       "      <th>liver_low</th>\n",
       "      <th>liver_high</th>\n",
       "      <th>spleen_low</th>\n",
       "      <th>spleen_high</th>\n",
       "      <th>kidney_low</th>\n",
       "      <th>kidney_high</th>\n",
       "      <th>extravasation_healthy</th>\n",
       "      <th>bowel_healthy</th>\n",
       "      <th>liver_healthy</th>\n",
       "      <th>spleen_healthy</th>\n",
       "      <th>kidney_healthy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48843</td>\n",
       "      <td>0.479187</td>\n",
       "      <td>0.299890</td>\n",
       "      <td>0.309901</td>\n",
       "      <td>0.224372</td>\n",
       "      <td>0.328844</td>\n",
       "      <td>0.290889</td>\n",
       "      <td>0.195177</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.520813</td>\n",
       "      <td>0.700110</td>\n",
       "      <td>0.465727</td>\n",
       "      <td>0.380267</td>\n",
       "      <td>0.511123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50046</td>\n",
       "      <td>0.485541</td>\n",
       "      <td>0.374966</td>\n",
       "      <td>0.274808</td>\n",
       "      <td>0.247720</td>\n",
       "      <td>0.303287</td>\n",
       "      <td>0.243375</td>\n",
       "      <td>0.209228</td>\n",
       "      <td>0.208284</td>\n",
       "      <td>0.514459</td>\n",
       "      <td>0.625034</td>\n",
       "      <td>0.477472</td>\n",
       "      <td>0.453338</td>\n",
       "      <td>0.582488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63706</td>\n",
       "      <td>0.379059</td>\n",
       "      <td>0.444275</td>\n",
       "      <td>0.298089</td>\n",
       "      <td>0.196277</td>\n",
       "      <td>0.412387</td>\n",
       "      <td>0.228513</td>\n",
       "      <td>0.220229</td>\n",
       "      <td>0.190025</td>\n",
       "      <td>0.620941</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.505634</td>\n",
       "      <td>0.359100</td>\n",
       "      <td>0.589746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  extravasation_injury  bowel_injury  liver_low  liver_high  \\\n",
       "0       48843              0.479187      0.299890   0.309901    0.224372   \n",
       "1       50046              0.485541      0.374966   0.274808    0.247720   \n",
       "2       63706              0.379059      0.444275   0.298089    0.196277   \n",
       "\n",
       "   spleen_low  spleen_high  kidney_low  kidney_high  extravasation_healthy  \\\n",
       "0    0.328844     0.290889    0.195177     0.293700               0.520813   \n",
       "1    0.303287     0.243375    0.209228     0.208284               0.514459   \n",
       "2    0.412387     0.228513    0.220229     0.190025               0.620941   \n",
       "\n",
       "   bowel_healthy  liver_healthy  spleen_healthy  kidney_healthy  \n",
       "0       0.700110       0.465727        0.380267        0.511123  \n",
       "1       0.625034       0.477472        0.453338        0.582488  \n",
       "2       0.555725       0.505634        0.359100        0.589746  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\")).drop(columns=target_cols)\n",
    "submission = submission.merge(df_submit, on=[\"patient_id\"], how=\"left\")\n",
    "submission = submission.fillna(0.0)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41199fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-13T15:18:38.500643Z",
     "iopub.status.busy": "2023-10-13T15:18:38.500369Z",
     "iopub.status.idle": "2023-10-13T15:18:38.507190Z",
     "shell.execute_reply": "2023-10-13T15:18:38.506361Z"
    },
    "papermill": {
     "duration": 0.016894,
     "end_time": "2023-10-13T15:18:38.509068",
     "exception": false,
     "start_time": "2023-10-13T15:18:38.492174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 121.340167,
   "end_time": "2023-10-13T15:18:41.893746",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-13T15:16:40.553579",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "50abf7d71d72401eb5de724fcf2d11f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "701788ea55cc4278bf607b09efa8f64a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c232ffa7ea943769472b6073277d452",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b94d465e05ba4a44ac0802a3b113ed63",
       "value": 1.0
      }
     },
     "9c232ffa7ea943769472b6073277d452": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "a1761fb4ef1a433f8986ea4f222d4ce9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a19b55ccee4f4d4594d7582289f1a3e2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d019d799c61349209f08eb6e2f329e08",
       "placeholder": "",
       "style": "IPY_MODEL_a1761fb4ef1a433f8986ea4f222d4ce9",
       "value": ""
      }
     },
     "b94d465e05ba4a44ac0802a3b113ed63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "c4ce7a67e32544549837e3e0e56e6398": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "cea6b44fdeef4d228986edf2f62c5728": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d019d799c61349209f08eb6e2f329e08": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e6c14e0de90641c2a0f6113dc116d4e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cea6b44fdeef4d228986edf2f62c5728",
       "placeholder": "",
       "style": "IPY_MODEL_c4ce7a67e32544549837e3e0e56e6398",
       "value": " 3/? [00:06&lt;00:00,  2.10s/it]"
      }
     },
     "f23a882761a64da5810452528a542b12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a19b55ccee4f4d4594d7582289f1a3e2",
        "IPY_MODEL_701788ea55cc4278bf607b09efa8f64a",
        "IPY_MODEL_e6c14e0de90641c2a0f6113dc116d4e8"
       ],
       "layout": "IPY_MODEL_50abf7d71d72401eb5de724fcf2d11f0"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
